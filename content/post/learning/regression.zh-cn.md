---
title: 机器学习：回归
description: 基于苏州大学黄海广老师课程的完整总结与心得
date: 2025-11-25
tags: [机器学习, 线性回归, 正则化, 梯度下降]
categories: [自学总结, 课程笔记]
image: /images/回归.png  # 可替换成一张回归相关的图
---


## 目录
- [目录](#目录)
- [1. 线性回归](#1-线性回归)
  - [损失函数公式及设计原因](#损失函数公式及设计原因)
  - [线性回归核心代码](#线性回归核心代码)
  - [线性最小二乘法 w（模型参数/权重）的计算](#线性最小二乘法-w模型参数权重的计算)
- [2. 梯度下降](#2-梯度下降)
  - [梯度下降的方法求 w 的三种形式](#梯度下降的方法求-w-的三种形式)
- [3. 数据归一化/标准化](#3-数据归一化标准化)
  - [为什么要标准化/归一化？](#为什么要标准化归一化)
  - [数据标准化对梯度下降的加速效果（经典图）](#数据标准化对梯度下降的加速效果经典图)
  - [归一化公式（Min-Max Scaling）](#归一化公式min-max-scaling)
  - [标准化公式（Z-score）](#标准化公式z-score)
  - [区别总结（一句话）](#区别总结一句话)
- [4. 正则化（解决过拟合）](#4-正则化解决过拟合)
  - [过拟合 vs 欠拟合](#过拟合-vs-欠拟合)
  - [正则化三大金刚](#正则化三大金刚)
- [5. 回归的指标评价](#5-回归的指标评价)

---

## 1. 线性回归

### 损失函数公式及设计原因

**线性回归损失函数公式：**

$$ J(w) = \frac{1}{2m} \sum_{i=1}^{m} (h_w(x^{(i)}) - y^{(i)})^2 $$


**为什么要乘 $\frac{1}{2}$？**
1. **求导时消掉平方前的 2**，让梯度公式更简洁  
2. 让损失函数的数学形式和**二次型标准形式对齐**（方便推导正规方程）

**为什么要除以 m（样本数）？**
1. 让损失等于所有样本的 **“平均误差”**（和统计学里的均方误差 MSE 完全一致）
2. 数值大小**不随样本数量变化**（100 条数据和 10 万条数据损失值差不多大，便于不同数据集比较）
3. **方便监控训练过程**（损失值直接就是一个“平均错误”）

### 线性回归核心代码

```python
def computeCost(X, y, w):
    inner = np.power(X @ w - y, 2)
    return np.sum(inner) / (2 * len(X))
```

### 线性最小二乘法 w（模型参数/权重）的计算

由于是线性的，所以 **w 是可以一次性计算出来的**

损失函数公式写成矩阵形式：

$$ J(w) = \frac{1}{2m}(Xw - y)^T (Xw - y) $$

因为这个函数是**强凸函数**，所以极值点就是最值点

我们要求：$\frac{\partial J}{\partial w} = 0$

<div align="center">
  <img src="/images/w参数推导.png" width="85%"/>
  <br/>
  <font color="gray">图：w参数推导过程</font><br>
</div>

所以最后得到的求 w 的公式为：

$$ w = (X^T X)^{-1} X^T y $$

**核心代码：**

```python
def LSM(X, y, w):
    return np.linalg.inv(X.T @ X) @ X.T @ y   # 一步到位
```

---

## 2. 梯度下降

**由于神经网络的损失函数是高度非凸的 → 没法解析求解，只能用梯度下降不停迭代，尽量靠近一个“足够好”的极小值。**

### 梯度下降的方法求 w 的三种形式

| 方式                        | 名称                               | 特点                                                                 |
|-----------------------------|------------------------------------|----------------------------------------------------------------------|
| **批量梯度下降**            | Batch Gradient Descent (BGD)       | 每一步都用到**所有的训练样本**                                       |
| **随机梯度下降**            | Stochastic Gradient Descent (SGD)  | 每一步只用**一个样本**，计算完立即更新参数                           |
| **小批量梯度下降**          | Mini-Batch Gradient Descent (MBGD) | 每一步用**一批样本**（常见 32~512），目前最主流                     |

**批量梯度下降参数更新公式：**

$$ w := w - \alpha \cdot \frac{1}{m} X^T (Xw - y) $$

**核心代码：**

```python
def batch_gradientDescent(X, y, w, alpha, count):
    costs = []
    for i in range(count):
        w = w - (X.T @ (X @ w - y)) * alpha / len(X)
        cost = computeCost(X, y, w)
        costs.append(cost)
        if i % 100 == 0:
            print("在第{}次迭代中，cost的值是：{}。".format(i, cost))
    return w, costs
```

**随机梯度下降（SGD）更新公式：**

$$ w := w - \alpha (x^{(i)}w - y^{(i)}) x^{(i)} $$

**小批量梯度下降（Mini-Batch）更新公式**：$$
w := w - \alpha \cdot \frac{1}{b} \sum_{i=t}^{t+b-1} (x^{(i)}w - y^{(i)}) x^{(i)}
$$

---

## 3. 数据归一化/标准化

### 为什么要标准化/归一化？

1. **提升模型精度**：不同维度之间的特征在数值上有一定比较性，可以大大提高分类器的准确性。
2. **加速模型收敛**：最优解的寻优过程明显会变得平缓，更容易正确地收敛到最优解。
### 数据标准化对梯度下降的加速效果（经典图）

<div align="center">
  <img src="/images/标准化对比.png" width="85%"/>
  <br/>
  <font color="gray">图：特征标准化前后梯度下降路径对比<br>
  左：未标准化（等高线严重拉伸，收敛慢且震荡）<br>
  右：标准化后（均值0，方差1，等高线接近圆形，收敛极快）</font>
</div>

### 归一化公式（Min-Max Scaling）

$$ x' = \frac{x - x_{\min}}{x_{\max} - x_{\min}} \quad \to [0, 1] $$

```python
# 归一化 → [0, 1]
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_norm = scaler.fit_transform(X)
```

### 标准化公式（Z-score）

$$ x' = \frac{x - \mu}{\sigma} \quad \to \text{均值0，标准差1} $$

```python
# 标准化 → 均值0，标准差1
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_std = scaler.fit_transform(X)
```

### 区别总结（一句话）

**归一化**：把数据强行塞进 [0,1] 盒子，**怕异常值**，适合有明确边界的情况。  
**标准化**：把数据变成“标准正态模样”（均值0，方差1），**对异常值相对鲁棒**，是机器学习中最常用、最安全的默认选择。

> **99% 的情况下，你直接用 StandardScaler（标准化）就完全没问题**，除非你明确知道自己在干什么（比如处理图像像素、或者明确要映射到 0~1）

---

## 4. 正则化（解决过拟合）

### 过拟合 vs 欠拟合

| 现象     | 形象比喻               | 处理方法                                      |
|----------|------------------------|-----------------------------------------------|
| **过拟合**   | 过于死记硬背，鲁棒性很差 | 获得更多数据、降维、正则化、集成学习          |
| **欠拟合**   | 根本没有学习，什么也不会 | 添加新特征、增加模型复杂度、减少正则化系数    |

### 正则化三大金刚

**L1 正则化（Lasso Regression）：**

$$ J(w) = MSE + \lambda \sum |w_j| $$

**L2 正则化（Ridge Regression）：**

$$ J(w) = MSE + \lambda \sum w_j^2 $$

**Elastic Net（弹性网络）：**

$$ J(w) = MSE + \lambda \rho \sum |w_j| + \lambda (1-\rho) \sum w_j^2 $$

其中：
- **λ** 为正则化系数，调整正则化项与训练误差的比例，**λ > 0**。
- **1 ≥ ρ ≥ 0** 为比例系数，调整 L1 与 L2 正则化的比例。

**它们仨都在干同一件事：**

在原来的“考试分数”（均方误差）后面，**额外罚你一笔钱**，逼你把不重要的 w 变小甚至归零，防止你学得太疯（过拟合）！

**形象比喻：**

- 没正则化：模型为了训练误差为 0，可以把 w 调到 10000、-50000（疯狂记住噪音）
- 加了 **L1（Lasso）**：每让 |w| 变大就被罚 λ×|w| → 模型：“太贵了，我还是把不重要的 w 设为 0 吧” → **稀疏解**
- 加了 **L2（Ridge）**：每让 w 变大就被罚 λ×w²（10000² = 1亿！）→ 模型：“我还是把所有 w 都压小一点吧” → **权重整体缩小**
- **Elastic Net**：两边都罚 → 最稳！

---

## 5. 回归的指标评价

| 指标                        | 公式                                                                 | 特点                                         |
|-----------------------------|----------------------------------------------------------------------|----------------------------------------------|
| **均方误差 MSE**            | $ MSE = \frac{1}{m} \sum_{i=1}^{m} (y^{(i)} - \hat{y}^{(i)})^2 $     | 对大误差惩罚更重                             |
| **均方根误差 RMSE**         | $ RMSE = \sqrt{MSE} $                                                | 与原数据单位一致，更直观                     |
| **平均绝对误差 MAE** | $ \text{MAE} = \dfrac{1}{m} \sum_{i=1}^{m} \left\| y^{(i)} - \hat{y}^{(i)} \right\| $ | 对异常值更鲁棒                               |
| **R²（R-Squared）**         | $ R^2\left(y,\hat{y}\right)=1-\frac{\sum_{i=1}^{m}\left(y^{\left(i\right)}-\widehat{y^{\left(i\right)}}\right)^2}{\sum_{i=1}^{m}\left(y^{\left(i\right)}-\bar{y}\right)^2}=1-\frac{\mathrm{SSE} }{\mathrm{SST}}=1-\frac{\mathrm{MSE} }{\mathrm{Var}\left(y\right)} $ | 越接近 1 越好，最大 1，可为负                |

其中：
- 残差平方和（SSE）：模型没解释的部分
- 回归平方和（SSR）：模型解释的部分
- 总平方和（SST）：数据的总波动

**一句话总结 R²：**

**R² = 1 − “模型犯的错” ÷ “啥也不干直接猜平均值犯的错”**

---
